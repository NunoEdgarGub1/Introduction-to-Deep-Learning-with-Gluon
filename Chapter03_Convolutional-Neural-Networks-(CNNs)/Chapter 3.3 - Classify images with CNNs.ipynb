{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3.3 - Classify images with CNNs\n",
    "\n",
    "* *Summary: End-to-end implementation and thought process for training a CNN to classify images*\n",
    "* *Duration: ~10 minutes (with NVIDIA 1080 Ti GPU)*\n",
    "* *Video: - (to be announced)*\n",
    "* *Slides: - (to be announced)*\n",
    "\n",
    "***\n",
    "\n",
    "In the last couple of years CNNs have been widely used both in research as in industry. The breaktrough of Convolutional Neural Networks was initiated by a number of research papers that outperformed human accuracy in the well known image competition ILSVRC (ImageNet Large-Scale Visual Recognition Challenge). In the competition, algorithms are scored for image classification and object detection at a large scale.\n",
    "\n",
    "CNNs are currently widely used in both research as real-world applications. This notebook will guide you through the steps of building a CNN for a image classification task. For this, we will use the MNIST dataset. In Chapter 2 we already introduced the MNIST dataset: a grayscale dataset with handwritten digits in the range from 0 to 9. With our neural network we were able to classify the images with 97.3% accuracy. In this notebook we will demonstate how a simple convolutional network can get an above-human accuracy of 99.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do?\n",
    "Classify handwritten digits (MNIST dataset) with a Convolutional Neural Network (CNN). In this notebook we pay special attention to guide you through the thought process for using a CNN to classify images. **_No assumptions are made._** Therefore, EDA, model selection, and hyperparameter tuning are all included separately. This to make sure you understand each design choice made and help you when applying CNNs to your own projects.\n",
    "\n",
    "## What do I need ([installation guide](https://github.com/indradenbakker/Introduction-to-Deep-Learning-with-Gluon))?\n",
    "* MXNet 1.0\n",
    "* NumPy & Matplotlib\n",
    "* Python 3.6\n",
    "* GPU (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## Code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. We start with importing Gluon and all dependencies needed to run the code in this notebook. In addition we set the seed for reproducability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. In this notebook we enable GPU usage by using the following line to set the context. If you don't have a GPU, you can replace '`gpu`' with '`cpu`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu() # mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The load the MNIST dataset we can use '`test_utils`' from MXNet as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.test_utils import get_mnist\n",
    "data = get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In many code examples for CNNs and the MNIST dataset one of the first steps is to load the MNIST dataset with a batch generator. This is done without any exploratory data analysis. This is highly unlikely to do in a real-world scenario. In this notebook we will follow the general steps without any assumptions, so that one can use this path for other problems.\n",
    "\n",
    "For other datasets you probably cannot use '`test_utils`'. Therefore, you can find code to load the raw MNIST dataset [here](https://github.com/indradenbakker/Introduction-to-Deep-Learning-with-Gluon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. To give us an idea of how the dataset looks like, let's plot some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 1, 28, 28)\n",
      "Training data labels: (60000,)\n",
      "Test data shape: (10000, 1, 28, 28)\n",
      "Test data labels: (10000,)\n",
      "Number of unique labels in training set: 10\n",
      "\n",
      "Training example: min: 0.0, max: 1.0, mean: 0.1681\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape: {}'.format(data['train_data'].shape))\n",
    "print('Training data labels: {}'.format(data['train_label'].shape))\n",
    "print('Test data shape: {}'.format(data['test_data'].shape))\n",
    "print('Test data labels: {}'.format(data['test_label'].shape))\n",
    "print('Number of unique labels in training set: {}'.format(len(set(data['train_label']))))\n",
    "\n",
    "# We randomly pick an example from the training set for some simple statistics\n",
    "data_example = data['train_data'][np.random.randint(0, len(data['train_data']))]\n",
    "print('\\nTraining example: min: {}, max: {}, mean: {:0.4f}'\n",
    "      .format(data_example.min(), data_example.max(), data_example.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learn from the above output is:\n",
    "* we have 60K training images and 10K test images\n",
    "* each datapoint (image) has a 3D-shape: (channel, width, height)\n",
    "* the images have shape 28x28\n",
    "* our images are in grayscale, so as expected we only have 1 channel\n",
    "* the image pixels are normalized and scaled: [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. We can also output a random example of each handwritten digit and their count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAB0CAYAAADw6hFAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6AJNREREREQkI/4f2Gw+IG5LTL8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dd4b0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = data['train_data']\n",
    "y_train = data['train_label']\n",
    "\n",
    "# Plot first image of each label\n",
    "unique_labels = set(data['train_label'])\n",
    "n_labels = len(unique_labels)\n",
    "plt.figure(figsize=(15, 15))\n",
    "i = 1\n",
    "for label in unique_labels:\n",
    "    image = X_train[y_train.tolist().index(label)]\n",
    "    plt.subplot(n_labels, n_labels, i)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{0}: ({1})\".format(label, y_train.tolist().count(label)))\n",
    "    i += 1\n",
    "    # To plot grayscale images with Matplotlib 2D-arrays are expected\n",
    "    # So we use image[0] to create a 2D-array from the (1, w, h) format.\n",
    "    _ = plt.imshow(image[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data is almost evenly distributed. Therefore, we will keep the dataset as is and we won't be using any preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. To evaluate our model during training we need to write a function. This function needs to iterate through the data and return the accuracy. As input, we will have a dataset iterator and the trained network.\n",
    "\n",
    "MXNet has a [Evaluation Metric API](https://mxnet.incubator.apache.org/api/python/metric.html) that includes all frequently used metrics, like accuracy and RMSE. \n",
    "\n",
    "In step x, we will show you how to set the data iterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. After a quick look at the training data we can start building our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=32, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))            \n",
    "    net.add(gluon.nn.Conv2D(channels=64, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(512, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple network architecture where we use 2 convolutional blocks with increasing number of channels. \n",
    "\n",
    "After the convolutional blocks, we flatten the output and feed the data to a fully connected layer with 512 units. \n",
    "\n",
    "We are trying to create a model to classify the handwritten digits. We have 10 unique labels in our training set, so our final layer should contain 10 units (1 for each label). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. As explained in Chapter 2, the initialisation of the weights can be important. For our network, we use the popular [Xavier initialisation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). In addtion, we use the defined context '`ctx`' to make sure the weights are placed on the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx) #magnitude=2.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Next, we need to define the loss function that we will use during training. For our multiclass classification problem, the Softmax cross-entropy loss is the logical choice. Softmax cross-entropy measures the probability error in a discrete classification tasks, where each training example only belongs to 1 class (the classes are mutually exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. We will be using the Adam optimizer to train our network. Adam is a great choice, if you're just starting and have no idea which hyperparameters to use. It will give you a great benchmark to build upon further in a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam') #'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now define the hyperparameters. We start with a relative low number of epochs, so that we can quickly see if our model is able to train correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "smoothing = .01\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of the loaded MNIST dataset, we will be using [Gluon dataset API](https://mxnet.incubator.apache.org/api/python/gluon/data.html)'s DataLoader to load and transform the data in batches while feeding the data to our network. First, we delete the loaded dataset to clean up the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# del data, data_example\n",
    "# num_inputs = 784\n",
    "# num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1)), label.astype(np.float32)\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are now ready to start training our model for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.0839653457669, Train_acc 0.985183333333, Test_acc 0.9815\n",
      "Epoch 1. Loss: 0.0626324687575, Train_acc 0.988, Test_acc 0.9824\n",
      "Epoch 2. Loss: 0.0514723012598, Train_acc 0.9873, Test_acc 0.981\n",
      "Epoch 3. Loss: 0.0398709181185, Train_acc 0.99235, Test_acc 0.9875\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])        \n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, curr_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! In just 4 epochs we were able to get a validation accuracy of 98.75%. This gives us confidence that our implementation works correctly and we can build further. \n",
    "\n",
    "What we see is the following:\n",
    "* Train accuracy is already 99.24% and by training our model for more epochs we can probably push the training accuracy further. \n",
    "* The validation accuracy stays a little bit behind: the difference in the 4th epoch is 0.485%. It's not a lot, but it can be a sign of overfitting on the training set. However, it's too soon to draw any conclusions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's implement early stopping to prevent overfitting on the training data and run our algorithm for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class earlyStopping():\n",
    "    def __init__(self, patience = 5, epoch_start = 0, best_loss = 1e100, minimize = True):\n",
    "        self.patience = patience\n",
    "        self.best_loss = best_loss if minimize else -best_loss\n",
    "        self.step = epoch_start\n",
    "        \n",
    "    def early_stopping(self, loss, e):\n",
    "        if (loss < self.best_loss):\n",
    "            self.step = 0\n",
    "            self.best_loss = loss\n",
    "        else:\n",
    "            self.step += 1\n",
    "        if self.step >= self.patience:\n",
    "            print(\"Early stopping after {} epochs with best loss {}\".format(e, self.best_loss))\n",
    "            return True\n",
    "        else: \n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now run our model with a callback for early stopping as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 0.00268501, Train_acc 0.991633333333, Test_acc 0.9833\n",
      "Epoch 2. Loss: 0.00401691, Train_acc 0.99405, Test_acc 0.988\n",
      "Epoch 3. Loss: 0.0121798, Train_acc 0.986733333333, Test_acc 0.9804\n",
      "Early stopping after 3 epochs with loss 0.012179761193692684\n",
      "Epoch 4. Loss: 0.000267027, Train_acc 0.9899, Test_acc 0.9839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-df1a579e276c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/indradenbakker/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/indradenbakker/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop = False\n",
    "cb = earlyStopping(patience = 2)\n",
    "e = 1\n",
    "\n",
    "while stop == False:\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0], )        \n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch {}. Loss: {}, Train_acc {}, Test_acc {}\".format(e, curr_loss, train_accuracy, test_accuracy))\n",
    "    stop = ES.early_stopping(curr_loss, e)\n",
    "    e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
